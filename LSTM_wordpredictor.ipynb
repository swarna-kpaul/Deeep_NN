{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"LSTM_wordpredictor.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1d0792dd50984ce288a09351c5f9e22a":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","state":{"_view_name":"TextView","style":"IPY_MODEL_b9fac07857d8454eb46c951e2375d2d0","_dom_classes":[],"description":"","_model_name":"TextModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"so she was considering in her own mind","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_02319fc0703d40408554b7326fc1a101"}},"b9fac07857d8454eb46c951e2375d2d0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"02319fc0703d40408554b7326fc1a101":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GkXTxvOIxlww","colab_type":"text"},"source":["# Build Next word predictor using LSTM"]},{"cell_type":"code","metadata":{"id":"KotVUGfYxlw6","colab_type":"code","outputId":"0e24d036-6059-4ff9-e35b-73ad152ed608","executionInfo":{"status":"ok","timestamp":1578459565227,"user_tz":-330,"elapsed":106788,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["########### import all libraries\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/My Drive/')\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","%matplotlib inline\n","import pickle\n","import numpy as np\n","from os import listdir\n","from os.path import isfile, join\n","import os\n","import numpy as np\n","import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout, Activation\n","from keras.optimizers import RMSprop, Adam\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_1FyTXtnxlxA","colab_type":"text"},"source":["## Create a function that will extract the data from the dataset\n","Next, we download the training data. The popular book “Alice’s Adventures in Wonderland” written by Lewis Caroll has been used as training dataset for this project. The e-book can be downloaded from http://www.gutenberg.org/files/11/11-0.txt in Plain Text UTF-8 format. The downloaded book has been stored in the root directory with the name ‘wonderland.txt’. We open this book using the open command and convert all characters into lowercase (so as to reduce the number of characters in the vocabulary, making it easier to learn for the model.)"]},{"cell_type":"code","metadata":{"id":"2BPEfCVey0cM","colab_type":"code","colab":{}},"source":["############### Download CIFAR-10 data and save it to google drive\n","import requests  \n","file_url = \"http://www.gutenberg.org/files/11/11-0.txt\"\n","    \n","r = requests.get(file_url, stream = True)  \n","  \n","with open(\"/content/drive/My Drive/DeepNN_HandsOn/Alice_in_won.txt\", \"wb\") as file:  \n","    for block in r.iter_content(chunk_size = 1024): \n","         if block:  \n","             file.write(block)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCs2rpm3xlxC","colab_type":"code","colab":{}},"source":["############# Read file data\n","file = open(\"/content/drive/My Drive/DeepNN_HandsOn/Alice_in_won.txt\", encoding = 'utf8')\n","raw_text = file.read()    #you need to read further characters as well\n","raw_text = raw_text.lower()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8teR2EhlxlxF","colab_type":"text"},"source":["## Creating Vocabulary\n","\n","Next, we store all the distinct characters occurring in the book in the chars variable. We also remove some of the rare characters (stored in bad-chars) from the book. The final vocabulary of the book is printed at the end of code segment."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"NPaBdiOqxlxH","colab_type":"code","outputId":"5c15a181-37b9-4e87-952d-7ca18e1e9b43","executionInfo":{"status":"ok","timestamp":1578459581502,"user_tz":-330,"elapsed":1230,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["chars = sorted(list(set(raw_text)))\n","print(chars)\n","bad_chars = ['#', '*', '@', '_', '\\ufeff']\n","for i in range(len(bad_chars)):\n","    raw_text = raw_text.replace(bad_chars[i],\"\")\n","\n","chars = sorted(list(set(raw_text)))\n","print(chars)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["['\\n', ' ', '!', '#', '$', '%', '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n","['\\n', ' ', '!', '$', '%', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7LENojSYnXlv","colab_type":"text"},"source":["# Summarize entire text"]},{"cell_type":"code","metadata":{"id":"iBmSTrm5xlxK","colab_type":"code","outputId":"5254e4ec-a8c6-4a73-9ae5-c934b3004f68","executionInfo":{"status":"ok","timestamp":1578459631278,"user_tz":-330,"elapsed":1253,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["text_length = len(raw_text)\n","char_length = len(chars)\n","VOCABULARY = char_length\n","SEQ_LENGTH =100\n","print(\"Text length = \" + str(text_length))\n","print(\"No. of characters = \" + str(char_length))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Text length = 163721\n","No. of characters = 56\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4TYiOICYnnpB","colab_type":"text"},"source":["# Preprocess data\n","\n","Now, we need to modify the dataset representation to bring it in the form the model will need. So, we create an input window of 100 characters (SEQ_LENGTH = 100) and shift the window one character at a time until we reach the end of the book. An encoding is used, so as to map each of the characters into it’s corresponding location in the vocabulary. Each time the input window contains a new sequence, it is converted into integers, using this encoding and appended to the input list of the dataset, input_strings. For all such input windows, the character just following the sequence is appended to the output list output_strings"]},{"cell_type":"code","metadata":{"id":"VuajbwX4xlxO","colab_type":"code","outputId":"0b5f4ddb-4491-4bd2-f802-db1105ee873d","executionInfo":{"status":"ok","timestamp":1578459709989,"user_tz":-330,"elapsed":4614,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","input_strings = []\n","output_strings = []\n","\n","for i in range(len(raw_text) - SEQ_LENGTH):\n","    X_text = raw_text[i: i + SEQ_LENGTH]\n","    X = [char_to_int[char] for char in X_text]\n","    input_strings.append(X)    \n","    Y = raw_text[i + SEQ_LENGTH]\n","    output_strings.append(char_to_int[Y])\n","# The input_strings and output_strings lists are converted into a numpy array of required dimensions, so that they can be fed to the model for the training.\n","length = len(input_strings)\n","input_strings = np.array(input_strings)\n","input_strings = np.reshape(input_strings, (input_strings.shape[0], input_strings.shape[1], 1))\n","input_strings = input_strings/float(VOCABULARY)\n","\n","output_strings = np.array(output_strings)\n","output_strings = np_utils.to_categorical(output_strings)\n","print(input_strings.shape)\n","print(output_strings.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(163621, 100, 1)\n","(163621, 56)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YM99mYFNoxLO","colab_type":"text"},"source":["# Design the Model\n","\n","We define the model now. The model is given an input of 100 character sequences and it outputs the respective probabilities with which a character can succeed the input sequence. The model consists of 3 hidden layers. The first two hidden layers consist of 256 LSTM cells, and the second layer is fully connected to the third layer. The number of neurons in the third layer is same as the number of unique characters in the training set (the vocabulary of the training set). The neurons in the third layer, use softmax activation so as to convert their outputs into respective probabilities. The loss used is Categorical cross entropy and the optimizer used is Adam."]},{"cell_type":"code","metadata":{"id":"PnT57-UvDpUy","colab_type":"code","colab":{}},"source":["def buildmodel(VOCABULARY):\n","    model = Sequential()\n","    model.add(LSTM(256, input_shape = (SEQ_LENGTH, 1), return_sequences = True))\n","    model.add(Dropout(0.2))\n","    model.add(LSTM(256))\n","    model.add(Dropout(0.2))\n","    model.add(Dense(VOCABULARY, activation = 'softmax'))\n","    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOLfKy1nxlxc","colab_type":"text"},"source":["## Running the model\n","\n","You can try different layouts to improve the accuracy"]},{"cell_type":"code","metadata":{"id":"6Q-SEJkhxlxe","colab_type":"code","outputId":"3d2d1b6b-783e-4909-c75b-616483ca4bb5","executionInfo":{"status":"ok","timestamp":1578420449063,"user_tz":-330,"elapsed":16525531,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["filepath=\"/content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]\n","model = buildmodel(VOCABULARY)\n","history = model.fit(input_strings, output_strings, epochs = 50, batch_size = 128, callbacks = callbacks_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/50\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","163621/163621 [==============================] - 329s 2ms/step - loss: 2.9077\n","\n","Epoch 00001: loss improved from inf to 2.90769, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-01-2.9077.hdf5\n","Epoch 2/50\n","163621/163621 [==============================] - 330s 2ms/step - loss: 2.6093\n","\n","Epoch 00002: loss improved from 2.90769 to 2.60927, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-02-2.6093.hdf5\n","Epoch 3/50\n","163621/163621 [==============================] - 331s 2ms/step - loss: 2.4226\n","\n","Epoch 00003: loss improved from 2.60927 to 2.42258, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-03-2.4226.hdf5\n","Epoch 4/50\n","163621/163621 [==============================] - 337s 2ms/step - loss: 2.2764\n","\n","Epoch 00004: loss improved from 2.42258 to 2.27642, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-04-2.2764.hdf5\n","Epoch 5/50\n","163621/163621 [==============================] - 338s 2ms/step - loss: 2.1703\n","\n","Epoch 00005: loss improved from 2.27642 to 2.17034, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-05-2.1703.hdf5\n","Epoch 6/50\n","163621/163621 [==============================] - 339s 2ms/step - loss: 2.0853\n","\n","Epoch 00006: loss improved from 2.17034 to 2.08528, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-06-2.0853.hdf5\n","Epoch 7/50\n","163621/163621 [==============================] - 341s 2ms/step - loss: 2.0171\n","\n","Epoch 00007: loss improved from 2.08528 to 2.01709, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-07-2.0171.hdf5\n","Epoch 8/50\n","163621/163621 [==============================] - 351s 2ms/step - loss: 1.9591\n","\n","Epoch 00008: loss improved from 2.01709 to 1.95910, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-08-1.9591.hdf5\n","Epoch 9/50\n","163621/163621 [==============================] - 349s 2ms/step - loss: 1.9088\n","\n","Epoch 00009: loss improved from 1.95910 to 1.90876, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-09-1.9088.hdf5\n","Epoch 10/50\n","163621/163621 [==============================] - 347s 2ms/step - loss: 1.8617\n","\n","Epoch 00010: loss improved from 1.90876 to 1.86175, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-10-1.8617.hdf5\n","Epoch 11/50\n","163621/163621 [==============================] - 342s 2ms/step - loss: 1.8219\n","\n","Epoch 00011: loss improved from 1.86175 to 1.82192, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-11-1.8219.hdf5\n","Epoch 12/50\n","163621/163621 [==============================] - 345s 2ms/step - loss: 1.7847\n","\n","Epoch 00012: loss improved from 1.82192 to 1.78470, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-12-1.7847.hdf5\n","Epoch 13/50\n","163621/163621 [==============================] - 349s 2ms/step - loss: 1.7498\n","\n","Epoch 00013: loss improved from 1.78470 to 1.74983, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-13-1.7498.hdf5\n","Epoch 14/50\n","163621/163621 [==============================] - 350s 2ms/step - loss: 1.7234\n","\n","Epoch 00014: loss improved from 1.74983 to 1.72344, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-14-1.7234.hdf5\n","Epoch 15/50\n","163621/163621 [==============================] - 349s 2ms/step - loss: 1.6930\n","\n","Epoch 00015: loss improved from 1.72344 to 1.69299, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-15-1.6930.hdf5\n","Epoch 16/50\n","163621/163621 [==============================] - 351s 2ms/step - loss: 1.6686\n","\n","Epoch 00016: loss improved from 1.69299 to 1.66861, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-16-1.6686.hdf5\n","Epoch 17/50\n","163621/163621 [==============================] - 350s 2ms/step - loss: 1.6462\n","\n","Epoch 00017: loss improved from 1.66861 to 1.64618, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-17-1.6462.hdf5\n","Epoch 18/50\n","163621/163621 [==============================] - 352s 2ms/step - loss: 1.6219\n","\n","Epoch 00018: loss improved from 1.64618 to 1.62193, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-18-1.6219.hdf5\n","Epoch 19/50\n","163621/163621 [==============================] - 353s 2ms/step - loss: 1.6019\n","\n","Epoch 00019: loss improved from 1.62193 to 1.60189, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-19-1.6019.hdf5\n","Epoch 20/50\n","163621/163621 [==============================] - 354s 2ms/step - loss: 1.5827\n","\n","Epoch 00020: loss improved from 1.60189 to 1.58273, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-20-1.5827.hdf5\n","Epoch 21/50\n","163621/163621 [==============================] - 353s 2ms/step - loss: 1.5629\n","\n","Epoch 00021: loss improved from 1.58273 to 1.56287, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-21-1.5629.hdf5\n","Epoch 22/50\n","163621/163621 [==============================] - 351s 2ms/step - loss: 1.5465\n","\n","Epoch 00022: loss improved from 1.56287 to 1.54650, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-22-1.5465.hdf5\n","Epoch 23/50\n","163621/163621 [==============================] - 350s 2ms/step - loss: 1.5324\n","\n","Epoch 00023: loss improved from 1.54650 to 1.53240, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-23-1.5324.hdf5\n","Epoch 24/50\n","163621/163621 [==============================] - 339s 2ms/step - loss: 1.5170\n","\n","Epoch 00024: loss improved from 1.53240 to 1.51703, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-24-1.5170.hdf5\n","Epoch 25/50\n","163621/163621 [==============================] - 333s 2ms/step - loss: 1.5010\n","\n","Epoch 00025: loss improved from 1.51703 to 1.50103, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-25-1.5010.hdf5\n","Epoch 26/50\n","163621/163621 [==============================] - 332s 2ms/step - loss: 1.4846\n","\n","Epoch 00026: loss improved from 1.50103 to 1.48455, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-26-1.4846.hdf5\n","Epoch 27/50\n","163621/163621 [==============================] - 332s 2ms/step - loss: 1.4880\n","\n","Epoch 00027: loss did not improve from 1.48455\n","Epoch 28/50\n","163621/163621 [==============================] - 331s 2ms/step - loss: 1.4539\n","\n","Epoch 00028: loss improved from 1.48455 to 1.45386, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-28-1.4539.hdf5\n","Epoch 29/50\n","163621/163621 [==============================] - 332s 2ms/step - loss: 1.4506\n","\n","Epoch 00029: loss improved from 1.45386 to 1.45058, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-29-1.4506.hdf5\n","Epoch 30/50\n","163621/163621 [==============================] - 331s 2ms/step - loss: 1.4443\n","\n","Epoch 00030: loss improved from 1.45058 to 1.44425, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-30-1.4443.hdf5\n","Epoch 31/50\n","163621/163621 [==============================] - 330s 2ms/step - loss: 1.4375\n","\n","Epoch 00031: loss improved from 1.44425 to 1.43750, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-31-1.4375.hdf5\n","Epoch 32/50\n","163621/163621 [==============================] - 334s 2ms/step - loss: 1.4305\n","\n","Epoch 00032: loss improved from 1.43750 to 1.43048, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-32-1.4305.hdf5\n","Epoch 33/50\n","163621/163621 [==============================] - 330s 2ms/step - loss: 1.4330\n","\n","Epoch 00033: loss did not improve from 1.43048\n","Epoch 34/50\n","163621/163621 [==============================] - 328s 2ms/step - loss: 1.4166\n","\n","Epoch 00034: loss improved from 1.43048 to 1.41664, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-34-1.4166.hdf5\n","Epoch 35/50\n","163621/163621 [==============================] - 335s 2ms/step - loss: 1.4009\n","\n","Epoch 00035: loss improved from 1.41664 to 1.40087, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-35-1.4009.hdf5\n","Epoch 36/50\n","163621/163621 [==============================] - 335s 2ms/step - loss: 1.3969\n","\n","Epoch 00036: loss improved from 1.40087 to 1.39688, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-36-1.3969.hdf5\n","Epoch 37/50\n","163621/163621 [==============================] - 336s 2ms/step - loss: 1.3900\n","\n","Epoch 00037: loss improved from 1.39688 to 1.39001, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-37-1.3900.hdf5\n","Epoch 38/50\n","163621/163621 [==============================] - 335s 2ms/step - loss: 1.3924\n","\n","Epoch 00038: loss did not improve from 1.39001\n","Epoch 39/50\n","163621/163621 [==============================] - 333s 2ms/step - loss: 1.3751\n","\n","Epoch 00039: loss improved from 1.39001 to 1.37509, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-39-1.3751.hdf5\n","Epoch 40/50\n","163621/163621 [==============================] - 336s 2ms/step - loss: 1.3759\n","\n","Epoch 00040: loss did not improve from 1.37509\n","Epoch 41/50\n","163621/163621 [==============================] - 338s 2ms/step - loss: 1.3681\n","\n","Epoch 00041: loss improved from 1.37509 to 1.36806, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-41-1.3681.hdf5\n","Epoch 42/50\n","163621/163621 [==============================] - 338s 2ms/step - loss: 1.3616\n","\n","Epoch 00042: loss improved from 1.36806 to 1.36156, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-42-1.3616.hdf5\n","Epoch 43/50\n","163621/163621 [==============================] - 337s 2ms/step - loss: 1.3555\n","\n","Epoch 00043: loss improved from 1.36156 to 1.35554, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-43-1.3555.hdf5\n","Epoch 44/50\n","163621/163621 [==============================] - 335s 2ms/step - loss: 1.3522\n","\n","Epoch 00044: loss improved from 1.35554 to 1.35216, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-44-1.3522.hdf5\n","Epoch 45/50\n","163621/163621 [==============================] - 337s 2ms/step - loss: 1.3475\n","\n","Epoch 00045: loss improved from 1.35216 to 1.34753, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-45-1.3475.hdf5\n","Epoch 46/50\n","163621/163621 [==============================] - 338s 2ms/step - loss: 1.3406\n","\n","Epoch 00046: loss improved from 1.34753 to 1.34059, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-46-1.3406.hdf5\n","Epoch 47/50\n","163621/163621 [==============================] - 341s 2ms/step - loss: 1.3386\n","\n","Epoch 00047: loss improved from 1.34059 to 1.33862, saving model to /content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-47-1.3386.hdf5\n","Epoch 48/50\n","128384/163621 [======================>.......] - ETA: 1:11 - loss: 1.3312Buffered data was truncated after reaching the output size limit."],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R8Y-uqdWxlxi","colab_type":"text"},"source":["## Next word Prediction\n","\n","The original model has been defined in a manner to take in 100 character inputs. So when the user initially starts typing the words, the total length of input string will be less than 100 characters. To solve this issue, the input has been padded with series of spaces in the beginning in order to make the total length of 100 characters. As the total length exceeds 100 characters, only last 100 characters are taken into consideration as the LSTM nodes take care of remembering the context of the document from before.\n","\n","Succeeding characters are predicted by the model until a space or full stop is encountered. The predicted characters are joined to form the next word, predicted by the mode"]},{"cell_type":"code","metadata":{"id":"hDM2iaxigz6l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":462},"outputId":"082fc586-5cbf-467e-a820-a4a305a422b0","executionInfo":{"status":"ok","timestamp":1578459648909,"user_tz":-330,"elapsed":11239,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}}},"source":["####### load weights\n","model = buildmodel(VOCABULARY)\n","model.load_weights(\"/content/drive/My Drive/DeepNN_HandsOn/LSTM_saved_models/weights-improvement-50-1.3195.hdf5\")"],"execution_count":8,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umjiY-MUxlxj","colab_type":"code","outputId":"31b8575e-1721-416e-c52d-078da7e2766c","executionInfo":{"status":"ok","timestamp":1578460524023,"user_tz":-330,"elapsed":1326,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["1d0792dd50984ce288a09351c5f9e22a","b9fac07857d8454eb46c951e2375d2d0","02319fc0703d40408554b7326fc1a101"]}},"source":["import ipywidgets as widgets\n","original_text = []\n","predicted_text = []\n","\n","text = widgets.Text()\n","display(text)\n","\n","def handle_submit(sender):\n","    global predicted_text\n","    global original_text\n","    \n","    inp = list(text.value)\n","    \n","    last_word = inp[len(original_text):]\n","    inp = inp[:len(original_text)]    \n","    original_text = text.value    \n","    last_word.append(' ')\n","    \n","    inp_text = [char_to_int[c] for c in inp]\n","    last_word = [char_to_int[c] for c in last_word]\n","    \n","    if len(inp_text) > 100:\n","        inp_text = inp_text[len(inp_text)-100: ]\n","    if len(inp_text) < 100:\n","        pad = []\n","        space = char_to_int[' ']\n","        pad = [space for i in range(100-len(inp_text))]\n","        inp_text = pad + inp_text\n","    \n","    while len(last_word)>0:\n","        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n","        next_char = model.predict(X/float(VOCABULARY))\n","        inp_text.append(last_word[0])\n","        inp_text = inp_text[1:]\n","        last_word.pop(0)\n","    \n","    next_word = []\n","    next_char = ':'\n","    while next_char != ' ':\n","        X = np.reshape(inp_text, (1, SEQ_LENGTH, 1))\n","        next_char = model.predict(X/float(VOCABULARY))\n","        index = np.argmax(next_char)        \n","        next_word.append(chars[index])\n","        inp_text.append(index)\n","        inp_text = inp_text[1:]\n","        next_char = chars[index]\n","    \n","    predicted_text = predicted_text + [''.join(next_word)]\n","    print(\" \" + ''.join(next_word), end='|')\n","    \n","text.on_submit(handle_submit)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d0792dd50984ce288a09351c5f9e22a","version_minor":0,"version_major":2},"text/plain":["Text(value='')"]},"metadata":{"tags":[]}},{"output_type":"stream","text":[" the | tp | fand | degan | and |"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"8cf97f6d-da34-4068-ef90-2a240ba44817","executionInfo":{"status":"ok","timestamp":1578389995707,"user_tz":-330,"elapsed":1649,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"id":"IIid0pRVHlFU","colab":{"base_uri":"https://localhost:8080/","height":300}},"source":["#view images\n","gen = datagen.flow(x_train, y_train, batch_size=64)\n","x,y = gen.next()\n","image = x[0].astype('uint')\n","plt.imshow(image)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f067f9f3fd0>"]},"metadata":{"tags":[]},"execution_count":72},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM6yZInNVvmsAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"EsX2u30Cxlxm","colab_type":"text"},"source":["## Compiling the model"]},{"cell_type":"code","metadata":{"id":"SiOYH81Rxlxn","colab_type":"code","colab":{}},"source":["# Compile the model\n","batch_size = 64\n","\n","opt_rms = keras.optimizers.rmsprop(lr=0.001, decay=1e-6)\n","model.compile(loss='categorical_crossentropy', \n","              optimizer=opt_rms, \n","              metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2l2ZsBPxlxr","colab_type":"text"},"source":["## Training the algorithm\n","I would suggest more than 100 epochs"]},{"cell_type":"code","metadata":{"id":"xwDle_bqxlxs","colab_type":"code","outputId":"3af66001-9067-495d-e4d1-6f68e526d3ec","executionInfo":{"status":"ok","timestamp":1578390479871,"user_tz":-330,"elapsed":474134,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["epochs = 20\n","model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n","                    steps_per_epoch=x_train.shape[0] // batch_size,\n","                    epochs=epochs,\n","                    verbose=1,\n","                    validation_data=(x_test, y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","781/781 [==============================] - 24s 31ms/step - loss: 1.5651 - acc: 0.4483 - val_loss: 1.6769 - val_acc: 0.4055\n","Epoch 2/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.5130 - acc: 0.4628 - val_loss: 1.3075 - val_acc: 0.5514\n","Epoch 3/20\n","781/781 [==============================] - 23s 30ms/step - loss: 1.4965 - acc: 0.4680 - val_loss: 1.7274 - val_acc: 0.4102\n","Epoch 4/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4838 - acc: 0.4785 - val_loss: 1.2818 - val_acc: 0.5297\n","Epoch 5/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4747 - acc: 0.4743 - val_loss: 1.3360 - val_acc: 0.5152\n","Epoch 6/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4550 - acc: 0.4838 - val_loss: 1.2076 - val_acc: 0.5693\n","Epoch 7/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4521 - acc: 0.4856 - val_loss: 1.3794 - val_acc: 0.5148\n","Epoch 8/20\n","781/781 [==============================] - 23s 30ms/step - loss: 1.4465 - acc: 0.4882 - val_loss: 1.3995 - val_acc: 0.4925\n","Epoch 9/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4456 - acc: 0.4858 - val_loss: 1.3218 - val_acc: 0.5198\n","Epoch 10/20\n","781/781 [==============================] - 24s 31ms/step - loss: 1.4390 - acc: 0.4883 - val_loss: 1.3035 - val_acc: 0.5309\n","Epoch 11/20\n","781/781 [==============================] - 24s 31ms/step - loss: 1.4519 - acc: 0.4827 - val_loss: 1.2799 - val_acc: 0.5362\n","Epoch 12/20\n","781/781 [==============================] - 24s 31ms/step - loss: 1.4412 - acc: 0.4846 - val_loss: 1.3298 - val_acc: 0.5434\n","Epoch 13/20\n","781/781 [==============================] - 23s 30ms/step - loss: 1.4398 - acc: 0.4872 - val_loss: 1.3294 - val_acc: 0.5431\n","Epoch 14/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4397 - acc: 0.4917 - val_loss: 1.3391 - val_acc: 0.5251\n","Epoch 15/20\n","781/781 [==============================] - 23s 30ms/step - loss: 1.4361 - acc: 0.4909 - val_loss: 1.2329 - val_acc: 0.5611\n","Epoch 16/20\n","781/781 [==============================] - 23s 30ms/step - loss: 1.4415 - acc: 0.4870 - val_loss: 1.2566 - val_acc: 0.5457\n","Epoch 17/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4465 - acc: 0.4866 - val_loss: 1.2861 - val_acc: 0.5430\n","Epoch 18/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4419 - acc: 0.4900 - val_loss: 1.2546 - val_acc: 0.5433\n","Epoch 19/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4494 - acc: 0.4837 - val_loss: 1.2192 - val_acc: 0.5719\n","Epoch 20/20\n","781/781 [==============================] - 24s 30ms/step - loss: 1.4388 - acc: 0.4889 - val_loss: 1.2163 - val_acc: 0.5633\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f067f9c0da0>"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"code","metadata":{"id":"a4EJGHsCxlxv","colab_type":"code","outputId":"9b7d5741-c724-4ec6-fd82-26fa23ccc3e3","executionInfo":{"status":"ok","timestamp":1578391132725,"user_tz":-330,"elapsed":787,"user":{"displayName":"swarna kamal Paul","photoUrl":"","userId":"16172308933059963349"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["out= model.predict(x_train[0:4])\n","out[3]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.0872292e-06, 1.0250151e-05, 1.9979252e-02, 1.2615286e-01,\n","       5.9632465e-02, 3.6577124e-02, 7.5343728e-01, 4.1993009e-03,\n","       3.2080740e-07, 1.0025548e-05], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":81}]}]}